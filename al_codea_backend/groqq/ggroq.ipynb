{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import bs4\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "load_dotenv()\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "## load the Groq API key\n",
    "groq_api_key = os.environ['GROQ_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh Kapoor\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ritesh Kapoor\\.cache\\huggingface\\hub\\models--BAAI--bge-base-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Ritesh Kapoor\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Just a moment...Enable JavaScript and cookies to continue', metadata={'source': 'https://www.datacamp.com/cheat-sheet/getting-started-with-python-cheat-sheet', 'content_type': 'text/html; charset=UTF-8', 'title': 'Just a moment...', 'language': 'en-US'})]\n"
     ]
    }
   ],
   "source": [
    "embeddings=hf\n",
    "url = \"https://www.datacamp.com/cheat-sheet/getting-started-with-python-cheat-sheet\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs)\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=250) \n",
    "final_documents=text_splitter.split_documents(docs) \n",
    "vectors=FAISS.from_documents(final_documents,embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm=ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "you are a coding test platform.\n",
    "I have give you a context of python documentation. \n",
    "You need to ask questions only on python language  <context>\n",
    "{context}\n",
    "<context>\n",
    "Questions:{input}\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retriever = vectors.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "print(fibonacci(10))\n",
      "\n",
      "a) 55\n",
      "b) 64\n",
      "c) 89\n",
      "\n",
      "---\n",
      "\n",
      "2. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "class A:\n",
      "    def __init__(self, value):\n",
      "        self.value = value\n",
      "\n",
      "    def __add__(self, other):\n",
      "        return self.value + other.value\n",
      "\n",
      "a = A(5)\n",
      "b = A(10)\n",
      "\n",
      "print(a + b)\n",
      "\n",
      "a) 15\n",
      "b) A(15)\n",
      "c) 510\n",
      "\n",
      "---\n",
      "\n",
      "3. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "\n",
      "print(list(filter(lambda x: x % 2 == 0, numbers)))\n",
      "\n",
      "a) [1, 3, 5]\n",
      "b) [2, 4]\n",
      "c) [1, 2, 3, 4, 5]\n",
      "\n",
      "---\n",
      "\n",
      "4. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "print(factorial(5))\n",
      "\n",
      "a) 120\n",
      "b) 125\n",
      "c) 150\n",
      "\n",
      "---\n",
      "\n",
      "5. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "def is_palindrome(word):\n",
      "    if len(word) <= 1:\n",
      "        return True\n",
      "    else:\n",
      "        if word[0] == word[-1]:\n",
      "            return is_palindrome(word[1:-1])\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "print(is_palindrome('racecar'))\n",
      "\n",
      "a) False\n",
      "b) True\n",
      "c) Error\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response=retrieval_chain.invoke({\"input\":'create 5 random challenging objective type question on python language generate only 3 options out of which 1 is correct. dont tell correct option     '})\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "## load the Groq API key\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "llm = Groq(model=\"mixtral-8x7b-32768\", api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: 1. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n <= 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "print(fibonacci(10))\n",
      "```\n",
      "\n",
      "a) 55\n",
      "b) 64\n",
      "c) 89\n",
      "\n",
      "---\n",
      "\n",
      "2. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "class A:\n",
      "    def __init__(self, value):\n",
      "        self.value = value\n",
      "\n",
      "    def __add__(self, other):\n",
      "        return self.value + other.value\n",
      "\n",
      "a = A(5)\n",
      "b = A(10)\n",
      "\n",
      "print(a + b)\n",
      "```\n",
      "\n",
      "a) 15\n",
      "b) A(15)\n",
      "c) 510\n",
      "\n",
      "---\n",
      "\n",
      "3. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "\n",
      "print(list(filter(lambda x: x % 2 == 0, numbers)))\n",
      "```\n",
      "\n",
      "a) [1, 3, 5]\n",
      "b) [2, 4]\n",
      "c) [1, 2, 3, 4, 5]\n",
      "\n",
      "---\n",
      "\n",
      "4. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "print(factorial(5))\n",
      "```\n",
      "\n",
      "a) 120\n",
      "b) 125\n",
      "c) 150\n",
      "\n",
      "---\n",
      "\n",
      "5. What is the output of the following code?\n",
      "\n",
      "```python\n",
      "def is_palindrome(word):\n",
      "    if len(word) <= 1:\n",
      "        return True\n",
      "    else:\n",
      "        if word[0] == word[-1]:\n",
      "            return is_palindrome(word[1:-1])\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "print(is_palindrome('racecar'))\n",
      "```\n",
      "\n",
      "a) False\n",
      "b) True\n",
      "c) Error\n",
      "\n",
      "Correct answers:\n",
      "\n",
      "1. a) 55\n",
      "2. a) 15\n",
      "3. b) [2, 4]\n",
      "4. a) 120\n",
      "5. b) True\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "answer=response\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"you are an intelligent ai assistant.answer the given 5 questions. if the correct answer is not amongst the options then add correct one  \"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=f\"{answer}\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
